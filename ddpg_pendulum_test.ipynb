{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gameagent import Agent\n",
    "import gym\n",
    "from ounoise import OUNoise\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "\n",
    "action_size = 1\n",
    "exploration_mu = 0\n",
    "exploration_theta = 0.15\n",
    "exploration_sigma = 0.25\n",
    "noise = OUNoise(action_size, exploration_mu, exploration_theta, exploration_sigma)\n",
    "\n",
    "def _compute_discounted_R(R, discount_rate=.999):\n",
    "    discounted_r = np.zeros_like(R, dtype=np.float32)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(len(R))):\n",
    "        running_add = running_add * discount_rate + R[t]\n",
    "        discounted_r[t] = running_add\n",
    "    discounted_r -= discounted_r.mean() \n",
    "    discounted_r /= discounted_r.std()\n",
    "\n",
    "    return discounted_r\n",
    "\n",
    "def compute_discounted_R(record,discounted_rate = 0.999):\n",
    "    reward_list = [x[2] for x in record]\n",
    "    reward_list = _compute_discounted_R(reward_list)\n",
    "    for i in range(len(record)):\n",
    "        record[i][2] = reward_list[i]\n",
    "    return record\n",
    "\n",
    "def run_process(iteration, double_mode = False, train = True, render = False,\\\n",
    "                train_batch_size = 128,verbose = False,reward_normalization = False,save_point = 10):\n",
    "    for iterate in range(iteration):\n",
    "        print('iterate : ',iterate)\n",
    "        if double_mode :\n",
    "            run_episode(train,render, train_batch_size, verbose, reward_normalization)\n",
    "            run_episode(False,render, train_batch_size, verbose, reward_normalization)\n",
    "        else:\n",
    "            run_episode(train,render,train_batch_size,verbose,reward_normalization)\n",
    "\n",
    "        if train & ((iterate+1) % save_point == 0):\n",
    "            #agent.main_critic.model.save_weights(\"./well_trained_main_critic_\"+str(iterate+1)+\".h5\")\n",
    "            #agent.target_critic.model.save_weights(\"./well_trained_target_critic_\"+str(iterate+1)+\".h5\")\n",
    "            #agent.main_actor.model.save_weights(\"./well_trained_main_actor_\"+str(iterate+1)+\".h5\") \n",
    "            #agent.target_actor.model.save_weights(\"./well_trained_target_actor_\"+str(iterate+1)+\".h5\")\n",
    "            print('saved')\n",
    "\n",
    "def run_episode(train = True, render = False, train_batch_size = 128,verbose = False,reward_normalization = False):\n",
    "    record = []\n",
    "    done = False\n",
    "    frame = env.reset()\n",
    "    ep_reward = 0\n",
    "    while done != True:\n",
    "        if render:\n",
    "            env.render()\n",
    "        state = frame.reshape(1,-1)\n",
    "        state = (state - env.observation_space.low) / \\\n",
    "                (env.observation_space.high - env.observation_space.low)\n",
    "\n",
    "        action = agent.get_action(state)\n",
    "        if train : \n",
    "            #action = np.clip((action*2 +(noise.sample())), -2, 2)\n",
    "            action = np.clip((action*2 +(np.random.normal())), -2, 2)\n",
    "        else :\n",
    "            action = np.clip(action, -1,1)\n",
    "        next_frame, reward, done, _ = env.step(action)\n",
    "        ep_reward += reward\n",
    "        record.append([state,action,ep_reward,next_frame.reshape(1,-1),done])\n",
    "        #record.append([state,action,reward,next_frame.reshape(1,-1),done])\n",
    "        #ep_reward += reward\n",
    "        frame = next_frame\n",
    "        if len(record) > 198 :\n",
    "            done = True\n",
    "        if verbose :\n",
    "            print('state : ', state, ', action :', action, ', reward : ', reward,', done : ',done,\\\n",
    "                ', ep_reward : ',ep_reward)\n",
    "        if done & train :\n",
    "            if reward_normalization : \n",
    "                record = compute_discounted_R(record)\n",
    "            list(map(lambda x : agent.memory.add(x[0],x[1],x[2],x[3],x[4]), record))\n",
    "        if (len(agent.memory)>train_batch_size * 10)& train:\n",
    "            print('trained_start')\n",
    "            agent.train()\n",
    "            print('trained_well')\n",
    "    print(\"ep_reward:\", ep_reward)\n",
    "    if train:\n",
    "        episode_reward_lst.append(ep_reward)\n",
    "    else:\n",
    "        test_episode_reward_lst.append(ep_reward)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = 'Pendulum-v0'\n",
    "batch_size = 128\n",
    "epochs = 100\n",
    "double_mode = True\n",
    "train = True\n",
    "render = False\n",
    "verbose = False\n",
    "reward_normalization= False\n",
    "save_point = 100\n",
    "episode_reward_lst = []\n",
    "test_episode_reward_lst = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(environment)\n",
    "agent = Agent(env.observation_space.shape[0],env.action_space.shape[0],train_batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterate :  0\n",
      "ep_reward: [-951.3926]\n",
      "ep_reward: [-1245.5569]\n",
      "iterate :  1\n",
      "ep_reward: [-1612.268]\n",
      "ep_reward: [-861.0428]\n",
      "iterate :  2\n",
      "ep_reward: [-1467.5261]\n",
      "ep_reward: [-1509.5052]\n",
      "iterate :  3\n",
      "ep_reward: [-1666.8035]\n",
      "ep_reward: [-1447.6425]\n",
      "iterate :  4\n",
      "ep_reward: [-1675.472]\n",
      "ep_reward: [-1328.2253]\n",
      "iterate :  5\n",
      "ep_reward: [-883.21375]\n",
      "ep_reward: [-1067.9294]\n",
      "iterate :  6\n",
      "trained_start\n",
      "trained_well\n",
      "ep_reward: [-1486.0402]\n",
      "ep_reward: [-1189.0046]\n",
      "iterate :  7\n",
      "ep_reward: [-1065.4954]\n",
      "ep_reward: [-1330.1123]\n",
      "iterate :  8\n",
      "ep_reward: [-1165.6113]\n",
      "ep_reward: [-1159.4467]\n",
      "iterate :  9\n",
      "ep_reward: [-1686.6218]\n",
      "ep_reward: [-1489.4423]\n",
      "iterate :  10\n",
      "ep_reward: [-1494.0138]\n",
      "ep_reward: [-745.04755]\n",
      "iterate :  11\n",
      "ep_reward: [-1699.4592]\n",
      "ep_reward: [-1086.0625]\n",
      "iterate :  12\n",
      "trained_start\n",
      "trained_well\n",
      "ep_reward: [-1001.25085]\n",
      "ep_reward: [-1503.9646]\n",
      "iterate :  13\n",
      "ep_reward: [-1641.8251]\n",
      "ep_reward: [-964.12964]\n",
      "iterate :  14\n",
      "ep_reward: [-1611.3851]\n",
      "ep_reward: [-1055.0001]\n",
      "iterate :  15\n",
      "ep_reward: [-1297.7959]\n",
      "ep_reward: [-1804.3264]\n",
      "iterate :  16\n",
      "ep_reward: [-872.8281]\n",
      "ep_reward: [-1065.8557]\n",
      "iterate :  17\n",
      "ep_reward: [-1004.58795]\n",
      "ep_reward: [-840.43]\n",
      "iterate :  18\n",
      "ep_reward: [-1195.4285]\n",
      "ep_reward: [-1042.4983]\n",
      "iterate :  19\n",
      "trained_start\n",
      "trained_well\n",
      "ep_reward: [-1108.4678]\n",
      "ep_reward: [-1453.7317]\n",
      "iterate :  20\n",
      "ep_reward: [-1170.9685]\n",
      "ep_reward: [-1331.0548]\n",
      "iterate :  21\n",
      "ep_reward: [-1111.8944]\n",
      "ep_reward: [-1768.7275]\n",
      "iterate :  22\n",
      "ep_reward: [-1055.9307]\n",
      "ep_reward: [-1249.054]\n",
      "iterate :  23\n",
      "ep_reward: [-1168.6752]\n",
      "ep_reward: [-1780.7164]\n",
      "iterate :  24\n",
      "ep_reward: [-1193.3192]\n",
      "ep_reward: [-1154.0437]\n",
      "iterate :  25\n",
      "ep_reward: [-1404.171]\n",
      "ep_reward: [-1080.9027]\n",
      "iterate :  26\n",
      "trained_start\n",
      "trained_well\n",
      "ep_reward: [-1233.8838]\n",
      "ep_reward: [-1194.3737]\n",
      "iterate :  27\n",
      "ep_reward: [-1286.6068]\n",
      "ep_reward: [-1664.0441]\n",
      "iterate :  28\n",
      "ep_reward: [-1441.2135]\n",
      "ep_reward: [-1065.6825]\n",
      "iterate :  29\n",
      "ep_reward: [-1271.3374]\n",
      "ep_reward: [-1804.019]\n",
      "iterate :  30\n",
      "ep_reward: [-1455.8387]\n",
      "ep_reward: [-1775.8582]\n",
      "iterate :  31\n",
      "ep_reward: [-1157.8446]\n",
      "ep_reward: [-1178.9216]\n",
      "iterate :  32\n",
      "trained_start\n",
      "trained_well\n",
      "ep_reward: [-1183.4977]\n",
      "ep_reward: [-1584.776]\n",
      "iterate :  33\n",
      "ep_reward: [-1086.5118]\n",
      "ep_reward: [-1402.4454]\n",
      "iterate :  34\n",
      "ep_reward: [-1310.1716]\n",
      "ep_reward: [-943.1553]\n",
      "iterate :  35\n",
      "ep_reward: [-1177.2217]\n",
      "ep_reward: [-907.6024]\n",
      "iterate :  36\n",
      "ep_reward: [-1188.8264]\n",
      "ep_reward: [-1274.1184]\n",
      "iterate :  37\n",
      "ep_reward: [-1153.4741]\n",
      "ep_reward: [-1006.26117]\n",
      "iterate :  38\n",
      "ep_reward: [-1233.263]\n",
      "ep_reward: [-857.18823]\n",
      "iterate :  39\n",
      "trained_start\n",
      "trained_well\n",
      "ep_reward: [-1067.4392]\n",
      "ep_reward: [-1068.7108]\n",
      "iterate :  40\n",
      "ep_reward: [-1176.2885]\n",
      "ep_reward: [-981.48083]\n",
      "iterate :  41\n",
      "ep_reward: [-1100.6969]\n",
      "ep_reward: [-967.56256]\n",
      "iterate :  42\n",
      "ep_reward: [-1412.9479]\n",
      "ep_reward: [-1323.8606]\n",
      "iterate :  43\n",
      "ep_reward: [-1110.4967]\n",
      "ep_reward: [-1754.3774]\n",
      "iterate :  44\n",
      "ep_reward: [-1155.0472]\n",
      "ep_reward: [-1472.9225]\n",
      "iterate :  45\n",
      "ep_reward: [-1370.412]\n",
      "ep_reward: [-1746.4999]\n",
      "iterate :  46\n",
      "trained_start\n",
      "trained_well\n",
      "ep_reward: [-1276.5635]\n",
      "ep_reward: [-966.31006]\n",
      "iterate :  47\n",
      "ep_reward: [-1100.9116]\n",
      "ep_reward: [-874.3035]\n",
      "iterate :  48\n",
      "ep_reward: [-1010.0472]\n",
      "ep_reward: [-1854.3347]\n",
      "iterate :  49\n",
      "ep_reward: [-1486.6804]\n",
      "ep_reward: [-1362.2383]\n",
      "iterate :  50\n",
      "ep_reward: [-1363.3448]\n",
      "ep_reward: [-1032.2339]\n",
      "iterate :  51\n",
      "ep_reward: [-1513.1476]\n",
      "ep_reward: [-1073.3555]\n",
      "iterate :  52\n",
      "ep_reward: [-1014.3808]\n",
      "ep_reward: [-1039.8284]\n",
      "iterate :  53\n",
      "trained_start\n",
      "trained_well\n",
      "ep_reward: [-1179.1859]\n",
      "ep_reward: [-1178.7366]\n",
      "iterate :  54\n",
      "ep_reward: [-1273.8456]\n",
      "ep_reward: [-1394.6107]\n",
      "iterate :  55\n",
      "ep_reward: [-1306.333]\n",
      "ep_reward: [-1320.4076]\n",
      "iterate :  56\n",
      "ep_reward: [-1181.4657]\n",
      "ep_reward: [-1428.7267]\n",
      "iterate :  57\n",
      "ep_reward: [-1023.917]\n",
      "ep_reward: [-1461.8627]\n",
      "iterate :  58\n",
      "ep_reward: [-1438.1823]\n",
      "ep_reward: [-1033.9163]\n",
      "iterate :  59\n",
      "trained_start\n",
      "trained_well\n",
      "ep_reward: [-1024.4211]\n",
      "ep_reward: [-1388.0325]\n",
      "iterate :  60\n",
      "ep_reward: [-1044.9053]\n",
      "ep_reward: [-1755.0514]\n",
      "iterate :  61\n",
      "ep_reward: [-1652.0712]\n",
      "ep_reward: [-1067.662]\n",
      "iterate :  62\n",
      "ep_reward: [-1450.8419]\n",
      "ep_reward: [-1384.4293]\n",
      "iterate :  63\n",
      "ep_reward: [-1309.2434]\n",
      "ep_reward: [-1338.5631]\n",
      "iterate :  64\n",
      "ep_reward: [-1168.2424]\n",
      "ep_reward: [-975.44135]\n",
      "iterate :  65\n",
      "ep_reward: [-1076.113]\n",
      "ep_reward: [-1670.65]\n",
      "iterate :  66\n",
      "trained_start\n",
      "trained_well\n",
      "ep_reward: [-1067.7139]\n",
      "ep_reward: [-1477.5912]\n",
      "iterate :  67\n",
      "ep_reward: [-1192.7247]\n",
      "ep_reward: [-1653.2997]\n",
      "iterate :  68\n",
      "ep_reward: [-1057.8264]\n",
      "ep_reward: [-1272.7307]\n",
      "iterate :  69\n",
      "ep_reward: [-1141.5356]\n",
      "ep_reward: [-1303.9114]\n",
      "iterate :  70\n",
      "ep_reward: [-1129.5303]\n",
      "ep_reward: [-1095.845]\n",
      "iterate :  71\n",
      "ep_reward: [-1039.1118]\n",
      "ep_reward: [-1280.2551]\n",
      "iterate :  72\n",
      "ep_reward: [-1731.8103]\n",
      "ep_reward: [-1186.5502]\n",
      "iterate :  73\n",
      "trained_start\n",
      "trained_well\n",
      "ep_reward: [-979.7688]\n",
      "ep_reward: [-1112.924]\n",
      "iterate :  74\n",
      "ep_reward: [-1713.4668]\n",
      "ep_reward: [-1070.7247]\n",
      "iterate :  75\n",
      "ep_reward: [-1277.819]\n",
      "ep_reward: [-1155.0715]\n",
      "iterate :  76\n",
      "ep_reward: [-1068.572]\n",
      "ep_reward: [-1419.9786]\n",
      "iterate :  77\n",
      "ep_reward: [-1562.6399]\n",
      "ep_reward: [-855.9545]\n",
      "iterate :  78\n",
      "ep_reward: [-1033.0392]\n",
      "ep_reward: [-993.65625]\n",
      "iterate :  79\n",
      "ep_reward: [-1273.9998]\n",
      "ep_reward: [-1430.4769]\n",
      "iterate :  80\n",
      "trained_start\n",
      "trained_well\n",
      "ep_reward: [-1165.9213]\n",
      "ep_reward: [-819.313]\n",
      "iterate :  81\n",
      "ep_reward: [-1698.2325]\n",
      "ep_reward: [-1080.5979]\n",
      "iterate :  82\n",
      "ep_reward: [-1169.9835]\n",
      "ep_reward: [-1285.2142]\n",
      "iterate :  83\n",
      "ep_reward: [-1657.9651]\n",
      "ep_reward: [-1245.067]\n",
      "iterate :  84\n",
      "ep_reward: [-1430.5283]\n",
      "ep_reward: [-1742.7435]\n",
      "iterate :  85\n",
      "ep_reward: [-1085.0747]\n",
      "ep_reward: [-978.5594]\n",
      "iterate :  86\n",
      "trained_start\n",
      "trained_well\n",
      "ep_reward: [-1040.9178]\n",
      "ep_reward: [-1068.4979]\n",
      "iterate :  87\n",
      "ep_reward: [-1208.8827]\n",
      "ep_reward: [-939.691]\n",
      "iterate :  88\n",
      "ep_reward: [-1067.7585]\n",
      "ep_reward: [-1345.6156]\n",
      "iterate :  89\n",
      "ep_reward: [-1168.351]\n",
      "ep_reward: [-1211.7104]\n",
      "iterate :  90\n",
      "ep_reward: [-1249.4786]\n",
      "ep_reward: [-1731.806]\n",
      "iterate :  91\n",
      "ep_reward: [-1066.2517]\n",
      "ep_reward: [-967.71246]\n",
      "iterate :  92\n",
      "ep_reward: [-1387.8889]\n",
      "ep_reward: [-1164.8645]\n",
      "iterate :  93\n",
      "trained_start\n",
      "trained_well\n",
      "ep_reward: [-953.43024]\n",
      "ep_reward: [-1198.9873]\n",
      "iterate :  94\n",
      "ep_reward: [-1435.9159]\n",
      "ep_reward: [-1304.1892]\n",
      "iterate :  95\n",
      "ep_reward: [-1010.60596]\n",
      "ep_reward: [-1616.1212]\n",
      "iterate :  96\n",
      "ep_reward: [-1038.0847]\n",
      "ep_reward: [-1059.1649]\n",
      "iterate :  97\n",
      "ep_reward: [-1303.9331]\n",
      "ep_reward: [-966.221]\n",
      "iterate :  98\n",
      "ep_reward: [-1064.3771]\n",
      "ep_reward: [-959.2869]\n",
      "iterate :  99\n",
      "ep_reward: [-962.18677]\n",
      "ep_reward: [-861.8636]\n",
      "saved\n"
     ]
    }
   ],
   "source": [
    "run_process(epochs,double_mode=double_mode, train=train,render = render, train_batch_size=batch_size,\\\n",
    "                verbose = verbose,reward_normalization=reward_normalization,save_point = save_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterate :  0\n",
      "ep_reward: [-1573.197]\n",
      "iterate :  1\n",
      "ep_reward: [-1907.1804]\n",
      "iterate :  2\n",
      "ep_reward: [-1061.6415]\n",
      "iterate :  3\n",
      "ep_reward: [-1152.4176]\n",
      "iterate :  4\n",
      "ep_reward: [-1203.1372]\n"
     ]
    }
   ],
   "source": [
    "run_process(5,double_mode=False, train=False,render = True, train_batch_size=batch_size,\\\n",
    "                verbose = verbose,reward_normalization=reward_normalization,save_point = save_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
